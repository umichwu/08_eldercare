import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

// å–å¾—ç•¶å‰æª”æ¡ˆçš„ç›®éŒ„
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// è¼‰å…¥ç’°å¢ƒè®Šæ•¸
// åœ¨æœ¬åœ°é–‹ç™¼ï¼šå¾æ ¹ç›®éŒ„çš„ .env è¼‰å…¥
// åœ¨ Renderï¼šç’°å¢ƒè®Šæ•¸å·²ç¶“åœ¨ Dashboard è¨­å®šï¼Œdotenv.config() ä¸æœƒè¦†è“‹ç¾æœ‰è®Šæ•¸
if (process.env.NODE_ENV !== 'production') {
  dotenv.config({ path: path.resolve(__dirname, '../../.env') });
} else {
  // ç”Ÿç”¢ç’°å¢ƒï¼šç’°å¢ƒè®Šæ•¸æ‡‰è©²ç”±å¹³å°æä¾›ï¼ˆRender Dashboardï¼‰
  dotenv.config(); // å˜—è©¦è¼‰å…¥ï¼Œä½†ä¸å¼·åˆ¶è¦æ±‚æª”æ¡ˆå­˜åœ¨
}

// æ”¯æŒçš„LLMæä¾›å•†
export const LLM_PROVIDERS = {
  OPENAI: 'openai',
  GEMINI: 'gemini',
  DEEPSEEK: 'deepseek'
};

// æ¯ä¸ªæä¾›å•†çš„é»˜è®¤æ¨¡å‹
const DEFAULT_MODELS = {
  [LLM_PROVIDERS.OPENAI]: 'gpt-4o-mini',
  [LLM_PROVIDERS.GEMINI]: 'gemini-2.0-flash-exp',
  [LLM_PROVIDERS.DEEPSEEK]: 'deepseek-chat'
};

// ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
const currentProvider = process.env.LLM_PROVIDER || LLM_PROVIDERS.GEMINI;
const openaiApiKey = process.env.OPENAI_API_KEY;
const geminiApiKey = process.env.GEMINI_API_KEY;
const deepseekApiKey = process.env.DEEPSEEK_API_KEY;

// åˆå§‹åŒ–å„ä¸ªLLMå®¢æˆ·ç«¯
let openaiClient = null;
let geminiClient = null;
let deepseekClient = null;

if (openaiApiKey) {
  openaiClient = new OpenAI({ apiKey: openaiApiKey });
  console.log('âœ… OpenAI client initialized');
}

if (geminiApiKey) {
  geminiClient = new GoogleGenerativeAI(geminiApiKey);
  console.log('âœ… Gemini client initialized');
}

if (deepseekApiKey) {
  deepseekClient = new OpenAI({
    apiKey: deepseekApiKey,
    baseURL: 'https://api.deepseek.com'
  });
  console.log('âœ… Deepseek client initialized');
}

// ç»Ÿä¸€çš„LLMæ¥å£
export class LLMService {
  constructor(provider = currentProvider) {
    this.provider = provider;
    this.client = this.getClient(provider);
    this.defaultModel = DEFAULT_MODELS[provider];
  }

  getClient(provider) {
    switch (provider) {
      case LLM_PROVIDERS.OPENAI:
        if (!openaiClient) {
          console.warn('âš ï¸  OpenAI API key not configured');
        }
        return openaiClient;
      case LLM_PROVIDERS.GEMINI:
        if (!geminiClient) {
          console.warn('âš ï¸  Gemini API key not configured');
        }
        return geminiClient;
      case LLM_PROVIDERS.DEEPSEEK:
        if (!deepseekClient) {
          console.warn('âš ï¸  Deepseek API key not configured');
        }
        return deepseekClient;
      default:
        console.error(`âŒ Unknown LLM provider: ${provider}`);
        return null;
    }
  }

  async generateResponse(messages, options = {}) {
    if (!this.client) {
      const errorMsg = `LLM client not initialized for provider: ${this.provider}`;
      console.error(`âŒ ${errorMsg}`);
      console.error(`   Available API Keys:`, {
        openai: !!openaiApiKey,
        gemini: !!geminiApiKey,
        deepseek: !!deepseekApiKey
      });
      throw new Error(errorMsg);
    }

    const temperature = options.temperature || 0.7;
    const maxTokens = options.maxTokens || 500;

    console.log(`ğŸ¤– Generating response with ${this.provider} (${this.defaultModel})`);
    console.log(`   Temperature: ${temperature}, MaxTokens: ${maxTokens}`);
    console.log(`   Messages count: ${messages.length}`);

    try {
      let result;
      if (this.provider === LLM_PROVIDERS.GEMINI) {
        result = await this.generateGeminiResponse(messages, temperature, maxTokens);
      } else {
        // OpenAI and Deepseek use the same API format
        result = await this.generateOpenAICompatibleResponse(messages, temperature, maxTokens);
      }

      console.log(`âœ… Response generated successfully from ${this.provider}`);
      console.log(`   Content length: ${result.content?.length || 0} chars`);
      return result;
    } catch (error) {
      console.error(`âŒ Error generating response from ${this.provider}:`, error.message);
      console.error(`   Error details:`, error);
      throw error;
    }
  }

  async generateOpenAICompatibleResponse(messages, temperature, maxTokens) {
    const completion = await this.client.chat.completions.create({
      model: this.defaultModel,
      messages: messages,
      temperature: temperature,
      max_tokens: maxTokens
    });

    return {
      content: completion.choices[0].message.content,
      usage: {
        promptTokens: completion.usage.prompt_tokens,
        completionTokens: completion.usage.completion_tokens,
        totalTokens: completion.usage.total_tokens
      }
    };
  }

  async generateGeminiResponse(messages, temperature, maxTokens) {
    const model = this.client.getGenerativeModel({
      model: this.defaultModel,
      generationConfig: {
        temperature: temperature,
        maxOutputTokens: maxTokens,
      }
    });

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
    const systemMessage = messages.find(m => m.role === 'system');
    const chatMessages = messages.filter(m => m.role !== 'system');

    // æ„å»ºèŠå¤©å†å²
    let history = chatMessages.slice(0, -1).map(msg => ({
      role: msg.role === 'assistant' ? 'model' : 'user',
      parts: [{ text: msg.content }]
    }));

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastMessage = chatMessages[chatMessages.length - 1];

    // å°†ç³»ç»Ÿæ¶ˆæ¯åˆå¹¶åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸­
    let prompt = lastMessage.content;
    if (systemMessage) {
      if (history.length === 0) {
        // å¦‚æœæ²¡æœ‰å†å²ï¼Œå°†ç³»ç»Ÿæ¶ˆæ¯æ·»åŠ åˆ°å½“å‰æ¶ˆæ¯å‰
        prompt = `${systemMessage.content}\n\nç”¨æˆ·é—®é¢˜ï¼š${prompt}`;
      } else {
        // å¦‚æœæœ‰å†å²ï¼Œå°†ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºå¯¹è¯å†å²çš„ç¬¬ä¸€ç»„äº¤äº’
        // æ³¨æ„ï¼šGemini API è¦æ±‚ç¬¬ä¸€æ¡æ¶ˆæ¯å¿…é¡»æ˜¯ user è§’è‰²
        const systemHistory = [
          {
            role: 'user',
            parts: [{ text: systemMessage.content }]
          },
          {
            role: 'model',
            parts: [{ text: 'å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚æˆ‘æœƒç”¨ç°¡å–®ã€è¦ªåˆ‡ã€æœ‰è€å¿ƒçš„èªæ°£ä¾†é™ªä¼´è€å¹´äººã€‚' }]
          }
        ];
        history = [...systemHistory, ...history];
      }
    }

    const chat = model.startChat({
      history: history
    });

    const result = await chat.sendMessage(prompt);
    const response = result.response;
    const text = response.text();

    return {
      content: text,
      usage: {
        promptTokens: 0, // Gemini API doesn't return token counts in the same way
        completionTokens: 0,
        totalTokens: 0
      }
    };
  }

  isAvailable() {
    return this.client !== null;
  }

  getProviderName() {
    return this.provider;
  }

  getModelName() {
    return this.defaultModel;
  }
}

// å¯¼å‡ºé»˜è®¤å®ä¾‹
export const defaultLLMService = new LLMService(currentProvider);

// å¯¼å‡ºè¾…åŠ©å‡½æ•°ç”¨äºåˆ›å»ºç‰¹å®šæä¾›å•†çš„å®ä¾‹
export function createLLMService(provider) {
  return new LLMService(provider);
}

// æ—¥å¿—å½“å‰é…ç½®
console.log('ğŸ“‹ LLM Configuration:');
console.log('   Current Provider:', currentProvider);
console.log('   Default Model:', DEFAULT_MODELS[currentProvider]);
console.log('   OpenAI API Key:', openaiApiKey ? 'Configured' : 'Not configured');
console.log('   Gemini API Key:', geminiApiKey ? 'Configured' : 'Not configured');
console.log('   Deepseek API Key:', deepseekApiKey ? 'Configured' : 'Not configured');
