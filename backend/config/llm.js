import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import geminiKeyPool from './geminiKeyPool.js';

// å–å¾—ç•¶å‰æª”æ¡ˆçš„ç›®éŒ„
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// è¼‰å…¥ç’°å¢ƒè®Šæ•¸
// åœ¨æœ¬åœ°é–‹ç™¼ï¼šå¾æ ¹ç›®éŒ„çš„ .env è¼‰å…¥
// åœ¨ Renderï¼šç’°å¢ƒè®Šæ•¸å·²ç¶“åœ¨ Dashboard è¨­å®šï¼Œdotenv.config() ä¸æœƒè¦†è“‹ç¾æœ‰è®Šæ•¸
if (process.env.NODE_ENV !== 'production') {
  dotenv.config({ path: path.resolve(__dirname, '../../.env') });
} else {
  // ç”Ÿç”¢ç’°å¢ƒï¼šç’°å¢ƒè®Šæ•¸æ‡‰è©²ç”±å¹³å°æä¾›ï¼ˆRender Dashboardï¼‰
  dotenv.config(); // å˜—è©¦è¼‰å…¥ï¼Œä½†ä¸å¼·åˆ¶è¦æ±‚æª”æ¡ˆå­˜åœ¨
}

// æ”¯æŒçš„LLMæä¾›å•†
export const LLM_PROVIDERS = {
  OPENAI: 'openai',
  GEMINI: 'gemini',
  DEEPSEEK: 'deepseek'
};

// æ¯ä¸ªæä¾›å•†çš„é»˜è®¤æ¨¡å‹
const DEFAULT_MODELS = {
  [LLM_PROVIDERS.OPENAI]: 'gpt-4o-mini',
  [LLM_PROVIDERS.GEMINI]: 'gemini-2.0-flash-exp',
  [LLM_PROVIDERS.DEEPSEEK]: 'deepseek-chat'
};

// ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
const currentProvider = process.env.LLM_PROVIDER || LLM_PROVIDERS.GEMINI;
const openaiApiKey = process.env.OPENAI_API_KEY;
const deepseekApiKey = process.env.DEEPSEEK_API_KEY;

// åˆå§‹åŒ–å„ä¸ªLLMå®¢æˆ·ç«¯
let openaiClient = null;
let deepseekClient = null;

if (openaiApiKey) {
  openaiClient = new OpenAI({ apiKey: openaiApiKey });
  console.log('âœ… OpenAI client initialized');
}

// âœ… Gemini ä½¿ç”¨ Key Poolï¼ˆæ”¯æŒå¤šå€‹ API Keysï¼‰
console.log('âœ… Gemini Key Pool initialized');

if (deepseekApiKey) {
  deepseekClient = new OpenAI({
    apiKey: deepseekApiKey,
    baseURL: 'https://api.deepseek.com'
  });
  console.log('âœ… Deepseek client initialized');
}

// ç»Ÿä¸€çš„LLMæ¥å£
export class LLMService {
  constructor(provider = currentProvider) {
    this.provider = provider;
    this.client = this.getClient(provider);
    this.defaultModel = DEFAULT_MODELS[provider];
  }

  getClient(provider) {
    switch (provider) {
      case LLM_PROVIDERS.OPENAI:
        if (!openaiClient) {
          console.warn('âš ï¸  OpenAI API key not configured');
        }
        return openaiClient;
      case LLM_PROVIDERS.GEMINI:
        // âœ… Gemini ä½¿ç”¨ Key Poolï¼Œåœ¨ generateGeminiResponse ä¸­å‹•æ…‹å–å¾—
        if (!geminiKeyPool.hasAvailableKeys()) {
          console.warn('âš ï¸  Gemini Key Pool ä¸­æ²’æœ‰å¯ç”¨çš„ Keys');
          return null;
        }
        return 'gemini-key-pool'; // è¿”å›ç‰¹æ®Šæ¨™è¨˜
      case LLM_PROVIDERS.DEEPSEEK:
        if (!deepseekClient) {
          console.warn('âš ï¸  Deepseek API key not configured');
        }
        return deepseekClient;
      default:
        console.error(`âŒ Unknown LLM provider: ${provider}`);
        return null;
    }
  }

  async generateResponse(messages, options = {}) {
    if (!this.client) {
      const errorMsg = `LLM client not initialized for provider: ${this.provider}`;
      console.error(`âŒ ${errorMsg}`);
      console.error(`   Available API Keys:`, {
        openai: !!openaiApiKey,
        gemini: !!geminiApiKey,
        deepseek: !!deepseekApiKey
      });
      throw new Error(errorMsg);
    }

    const temperature = options.temperature || 0.7;
    const maxTokens = options.maxTokens || 500;

    console.log(`ğŸ¤– Generating response with ${this.provider} (${this.defaultModel})`);
    console.log(`   Temperature: ${temperature}, MaxTokens: ${maxTokens}`);
    console.log(`   Messages count: ${messages.length}`);

    try {
      let result;
      if (this.provider === LLM_PROVIDERS.GEMINI) {
        result = await this.generateGeminiResponse(messages, temperature, maxTokens);
      } else {
        // OpenAI and Deepseek use the same API format
        result = await this.generateOpenAICompatibleResponse(messages, temperature, maxTokens);
      }

      console.log(`âœ… Response generated successfully from ${this.provider}`);
      console.log(`   Content length: ${result.content?.length || 0} chars`);
      return result;
    } catch (error) {
      console.error(`âŒ Error generating response from ${this.provider}:`, error.message);
      console.error(`   Error details:`, error);
      throw error;
    }
  }

  async generateOpenAICompatibleResponse(messages, temperature, maxTokens) {
    const completion = await this.client.chat.completions.create({
      model: this.defaultModel,
      messages: messages,
      temperature: temperature,
      max_tokens: maxTokens
    });

    return {
      content: completion.choices[0].message.content,
      usage: {
        promptTokens: completion.usage.prompt_tokens,
        completionTokens: completion.usage.completion_tokens,
        totalTokens: completion.usage.total_tokens
      }
    };
  }

  async generateGeminiResponse(messages, temperature, maxTokens) {
    // âœ… ä½¿ç”¨ Key Pool ç²å–å¯ç”¨çš„ Clientï¼ˆæ”¯æŒé‡è©¦ï¼‰
    const { client, keyInfo } = await geminiKeyPool.getClientWithRetry();

    try {
      const model = client.getGenerativeModel({
        model: this.defaultModel,
        generationConfig: {
          temperature: temperature,
          maxOutputTokens: maxTokens,
        }
      });

      // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
      const systemMessage = messages.find(m => m.role === 'system');
      const chatMessages = messages.filter(m => m.role !== 'system');

      // æ„å»ºèŠå¤©å†å²
      let history = chatMessages.slice(0, -1).map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }));

      // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
      const lastMessage = chatMessages[chatMessages.length - 1];

      // å°†ç³»ç»Ÿæ¶ˆæ¯åˆå¹¶åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸­
      let prompt = lastMessage.content;
      if (systemMessage) {
        if (history.length === 0) {
          // å¦‚æœæ²¡æœ‰å†å²ï¼Œå°†ç³»ç»Ÿæ¶ˆæ¯æ·»åŠ åˆ°å½“å‰æ¶ˆæ¯å‰
          prompt = `${systemMessage.content}\n\nç”¨æˆ·é—®é¢˜ï¼š${prompt}`;
        } else {
          // å¦‚æœæœ‰å†å²ï¼Œå°†ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºå¯¹è¯å†å²çš„ç¬¬ä¸€ç»„äº¤äº’
          // æ³¨æ„ï¼šGemini API è¦æ±‚ç¬¬ä¸€æ¡æ¶ˆæ¯å¿…é¡»æ˜¯ user è§’è‰²
          const systemHistory = [
            {
              role: 'user',
              parts: [{ text: systemMessage.content }]
            },
            {
              role: 'model',
              parts: [{ text: 'å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚æˆ‘æœƒç”¨ç°¡å–®ã€è¦ªåˆ‡ã€æœ‰è€å¿ƒçš„èªæ°£ä¾†é™ªä¼´è€å¹´äººã€‚' }]
            }
          ];
          history = [...systemHistory, ...history];
        }
      }

      const chat = model.startChat({
        history: history
      });

      const result = await chat.sendMessage(prompt);
      const response = result.response;
      const text = response.text();

      // âœ… æ¨™è¨˜ç‚ºæˆåŠŸ
      geminiKeyPool.markSuccess(keyInfo);

      return {
        content: text,
        usage: {
          promptTokens: 0, // Gemini API doesn't return token counts in the same way
          completionTokens: 0,
          totalTokens: 0
        }
      };
    } catch (error) {
      // âœ… æª¢æŸ¥æ˜¯å¦ç‚ºé…é¡éŒ¯èª¤
      const isQuotaError = error.message && (
        error.message.includes('quota') ||
        error.message.includes('429') ||
        error.message.includes('Too Many Requests') ||
        error.message.includes('RESOURCE_EXHAUSTED')
      );

      if (isQuotaError) {
        console.warn(`âŒ ${keyInfo.id} é…é¡éŒ¯èª¤ï¼Œæ¨™è¨˜ç‚ºä¸å¯ç”¨`);
        geminiKeyPool.markQuotaError(keyInfo);

        // é‡è©¦ï¼šå˜—è©¦ä½¿ç”¨ä¸‹ä¸€å€‹ Key
        if (geminiKeyPool.hasAvailableKeys()) {
          console.log('ğŸ”„ å˜—è©¦ä½¿ç”¨ä¸‹ä¸€å€‹å¯ç”¨çš„ API Key...');
          return this.generateGeminiResponse(messages, temperature, maxTokens);
        }
      } else {
        geminiKeyPool.markError(keyInfo, error);
      }

      throw error;
    }
  }

  isAvailable() {
    // âœ… ç‰¹æ®Šè™•ç† Gemini Key Pool
    if (this.provider === LLM_PROVIDERS.GEMINI) {
      return geminiKeyPool.hasAvailableKeys();
    }
    return this.client !== null;
  }

  getProviderName() {
    return this.provider;
  }

  getModelName() {
    return this.defaultModel;
  }
}

// å¯¼å‡ºé»˜è®¤å®ä¾‹
export const defaultLLMService = new LLMService(currentProvider);

// å¯¼å‡ºè¾…åŠ©å‡½æ•°ç”¨äºåˆ›å»ºç‰¹å®šæä¾›å•†çš„å®ä¾‹
export function createLLMService(provider) {
  return new LLMService(provider);
}

// æ—¥å¿—å½“å‰é…ç½®
console.log('ğŸ“‹ LLM Configuration:');
console.log('   Current Provider:', currentProvider);
console.log('   Default Model:', DEFAULT_MODELS[currentProvider]);
console.log('   OpenAI API Key:', openaiApiKey ? 'Configured' : 'Not configured');
console.log('   Gemini Key Pool:', geminiKeyPool.keys.length > 0 ? `${geminiKeyPool.keys.length} Keys` : 'Not configured');
console.log('   Deepseek API Key:', deepseekApiKey ? 'Configured' : 'Not configured');

// é¡¯ç¤º Gemini Key Pool è©³ç´°è³‡è¨Š
if (geminiKeyPool.keys.length > 0) {
  console.log('\nğŸ”‘ Gemini API Key Pool Status:');
  geminiKeyPool.keys.forEach((key, index) => {
    console.log(`   #${index + 1} ${key.id}: ${key.prefix} - ${key.isHealthy ? 'âœ… Healthy' : 'âŒ Unhealthy'}`);
  });
}
