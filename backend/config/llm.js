import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

// å–å¾—ç•¶å‰æª”æ¡ˆçš„ç›®éŒ„
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// è¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼ˆå‘ä¸Šå…©å±¤ = å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

// æ”¯æŒçš„LLMæä¾›å•†
export const LLM_PROVIDERS = {
  OPENAI: 'openai',
  GEMINI: 'gemini',
  DEEPSEEK: 'deepseek'
};

// æ¯ä¸ªæä¾›å•†çš„é»˜è®¤æ¨¡å‹
const DEFAULT_MODELS = {
  [LLM_PROVIDERS.OPENAI]: 'gpt-4o-mini',
  [LLM_PROVIDERS.GEMINI]: 'gemini-2.0-flash-exp',
  [LLM_PROVIDERS.DEEPSEEK]: 'deepseek-chat'
};

// ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
const currentProvider = process.env.LLM_PROVIDER || LLM_PROVIDERS.GEMINI;
const openaiApiKey = process.env.OPENAI_API_KEY;
const geminiApiKey = process.env.GEMINI_API_KEY;
const deepseekApiKey = process.env.DEEPSEEK_API_KEY;

// åˆå§‹åŒ–å„ä¸ªLLMå®¢æˆ·ç«¯
let openaiClient = null;
let geminiClient = null;
let deepseekClient = null;

if (openaiApiKey) {
  openaiClient = new OpenAI({ apiKey: openaiApiKey });
  console.log('âœ… OpenAI client initialized');
}

if (geminiApiKey) {
  geminiClient = new GoogleGenerativeAI(geminiApiKey);
  console.log('âœ… Gemini client initialized');
}

if (deepseekApiKey) {
  deepseekClient = new OpenAI({
    apiKey: deepseekApiKey,
    baseURL: 'https://api.deepseek.com'
  });
  console.log('âœ… Deepseek client initialized');
}

// ç»Ÿä¸€çš„LLMæ¥å£
export class LLMService {
  constructor(provider = currentProvider) {
    this.provider = provider;
    this.client = this.getClient(provider);
    this.defaultModel = DEFAULT_MODELS[provider];
  }

  getClient(provider) {
    switch (provider) {
      case LLM_PROVIDERS.OPENAI:
        if (!openaiClient) {
          console.warn('âš ï¸  OpenAI API key not configured');
        }
        return openaiClient;
      case LLM_PROVIDERS.GEMINI:
        if (!geminiClient) {
          console.warn('âš ï¸  Gemini API key not configured');
        }
        return geminiClient;
      case LLM_PROVIDERS.DEEPSEEK:
        if (!deepseekClient) {
          console.warn('âš ï¸  Deepseek API key not configured');
        }
        return deepseekClient;
      default:
        console.error(`âŒ Unknown LLM provider: ${provider}`);
        return null;
    }
  }

  async generateResponse(messages, options = {}) {
    if (!this.client) {
      throw new Error(`LLM client not initialized for provider: ${this.provider}`);
    }

    const temperature = options.temperature || 0.7;
    const maxTokens = options.maxTokens || 500;

    try {
      if (this.provider === LLM_PROVIDERS.GEMINI) {
        return await this.generateGeminiResponse(messages, temperature, maxTokens);
      } else {
        // OpenAI and Deepseek use the same API format
        return await this.generateOpenAICompatibleResponse(messages, temperature, maxTokens);
      }
    } catch (error) {
      console.error(`âŒ Error generating response from ${this.provider}:`, error);
      throw error;
    }
  }

  async generateOpenAICompatibleResponse(messages, temperature, maxTokens) {
    const completion = await this.client.chat.completions.create({
      model: this.defaultModel,
      messages: messages,
      temperature: temperature,
      max_tokens: maxTokens
    });

    return {
      content: completion.choices[0].message.content,
      usage: {
        promptTokens: completion.usage.prompt_tokens,
        completionTokens: completion.usage.completion_tokens,
        totalTokens: completion.usage.total_tokens
      }
    };
  }

  async generateGeminiResponse(messages, temperature, maxTokens) {
    const model = this.client.getGenerativeModel({
      model: this.defaultModel,
      generationConfig: {
        temperature: temperature,
        maxOutputTokens: maxTokens,
      }
    });

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
    const systemMessage = messages.find(m => m.role === 'system');
    const chatMessages = messages.filter(m => m.role !== 'system');

    // æ„å»ºèŠå¤©å†å²
    const history = chatMessages.slice(0, -1).map(msg => ({
      role: msg.role === 'assistant' ? 'model' : 'user',
      parts: [{ text: msg.content }]
    }));

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastMessage = chatMessages[chatMessages.length - 1];

    // å¦‚æœæœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œå°†å…¶æ·»åŠ åˆ°ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸­
    let prompt = lastMessage.content;
    if (systemMessage && history.length === 0) {
      prompt = `${systemMessage.content}\n\n${prompt}`;
    }

    const chat = model.startChat({
      history: history,
      systemInstruction: systemMessage ? systemMessage.content : undefined
    });

    const result = await chat.sendMessage(prompt);
    const response = result.response;
    const text = response.text();

    return {
      content: text,
      usage: {
        promptTokens: 0, // Gemini API doesn't return token counts in the same way
        completionTokens: 0,
        totalTokens: 0
      }
    };
  }

  isAvailable() {
    return this.client !== null;
  }

  getProviderName() {
    return this.provider;
  }

  getModelName() {
    return this.defaultModel;
  }
}

// å¯¼å‡ºé»˜è®¤å®ä¾‹
export const defaultLLMService = new LLMService(currentProvider);

// å¯¼å‡ºè¾…åŠ©å‡½æ•°ç”¨äºåˆ›å»ºç‰¹å®šæä¾›å•†çš„å®ä¾‹
export function createLLMService(provider) {
  return new LLMService(provider);
}

// æ—¥å¿—å½“å‰é…ç½®
console.log('ğŸ“‹ LLM Configuration:');
console.log('   Current Provider:', currentProvider);
console.log('   Default Model:', DEFAULT_MODELS[currentProvider]);
console.log('   OpenAI API Key:', openaiApiKey ? 'Configured' : 'Not configured');
console.log('   Gemini API Key:', geminiApiKey ? 'Configured' : 'Not configured');
console.log('   Deepseek API Key:', deepseekApiKey ? 'Configured' : 'Not configured');
